{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX8vyeuWCYZn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "from scipy import stats as st\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK38w-xSpwmn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Global constants. Note that you will change  rare doc frequency threshold and max_features will be programmatically as part of hyperparameter optimization. I've set a default value for testing purposes\n",
        "\n",
        "\n",
        "\n",
        "ENGLISH_STOP_WORDS_MODIFIED =[  # List taken from sklearn's defaults and adjusted according to manual inspection of data and prior beliefs about predictive words \n",
        "    'a',\n",
        "    'about',\n",
        "    'above',\n",
        "    'across',\n",
        "    'after',\n",
        "    'afterwards',\n",
        "    #'again',\n",
        "    'against',\n",
        "    'ain',\n",
        "    'all',\n",
        "    #'almost',\n",
        "    #'alone',\n",
        "    'along',\n",
        "    #'already',\n",
        "    #'also',\n",
        "    #'although',\n",
        "    'always',\n",
        "    'am',\n",
        "    'among',\n",
        "    'amongst',\n",
        "    'amoungst',\n",
        "    'amount',\n",
        "    'an',\n",
        "    'and',\n",
        "    #'another',\n",
        "    #'any',\n",
        "    'anyhow',\n",
        "    #'anyone',\n",
        "    #'anything',\n",
        "    'anyway',\n",
        "    #'anywhere',\n",
        "    'are',\n",
        "    'aren',\n",
        "    'around',\n",
        "    'as',\n",
        "    'at',\n",
        "    'back',\n",
        "    'be',\n",
        "    'became',\n",
        "    'because',\n",
        "    'become',\n",
        "    'becomes',\n",
        "    'becoming',\n",
        "    'been',\n",
        "    #'before',\n",
        "    'beforehand',\n",
        "    #'behind',\n",
        "    'being',\n",
        "    #'below',\n",
        "    'beside',\n",
        "    'besides',\n",
        "    'between',\n",
        "    #'beyond',\n",
        "    'bill',\n",
        "    'both',\n",
        "    'bottom',\n",
        "    'but',\n",
        "    'by',\n",
        "    #'call',\n",
        "    'can',\n",
        "    #'cannot',\n",
        "    #'cant',\n",
        "    'co',\n",
        "    'con',\n",
        "    #'could',\n",
        "    #'couldn',\n",
        "    #'couldnt',\n",
        "    'cry',\n",
        "    'd',\n",
        "    'de',\n",
        "    #'describe',\n",
        "    #'detail',\n",
        "    #'did',\n",
        "    #'didn',\n",
        "    'do',\n",
        "    #'does',\n",
        "    #'doesn',\n",
        "    'doing',\n",
        "    'don',\n",
        "    #'done',\n",
        "    'down',\n",
        "    'due',\n",
        "    'during',\n",
        "    'each',\n",
        "    'eg',\n",
        "    'eight',\n",
        "    'either',\n",
        "    'eleven',\n",
        "    'else',\n",
        "    'elsewhere',\n",
        "    'empty',\n",
        "    #'enough',\n",
        "    'etc',\n",
        "    'even',\n",
        "    'ever',\n",
        "    'every',\n",
        "    'everyone',\n",
        "    'everything',\n",
        "    'everywhere',\n",
        "    #'except',\n",
        "    #'few',\n",
        "    'fifteen',\n",
        "    'fify',\n",
        "    'fill',\n",
        "    #'find',\n",
        "    'fire',\n",
        "    #'first',\n",
        "    'five',\n",
        "    'for',\n",
        "    #'former',\n",
        "    #'formerly',\n",
        "    'forty',\n",
        "    'found',\n",
        "    'four',\n",
        "    'from',\n",
        "    'front',\n",
        "    'full',\n",
        "    #'further',\n",
        "    'get',\n",
        "    'give',\n",
        "    'go',\n",
        "    'had',\n",
        "    'hadn',\n",
        "    #'has',\n",
        "    #'hasn',\n",
        "    #'hasnt',\n",
        "    'have',\n",
        "    'haven',\n",
        "    'having',\n",
        "    'he',\n",
        "    'hence',\n",
        "    'her',\n",
        "    'here',\n",
        "    'hereafter',\n",
        "    'hereby',\n",
        "    'herein',\n",
        "    'hereupon',\n",
        "    'hers',\n",
        "    'herself',\n",
        "    'him',\n",
        "    'himself',\n",
        "    'his',\n",
        "    'how',\n",
        "    #'however',\n",
        "    'hundred',\n",
        "    'i',\n",
        "    'ie',\n",
        "    'if',\n",
        "    'in',\n",
        "    'inc',\n",
        "    'indeed',\n",
        "    'interest',\n",
        "    'into',\n",
        "    'is',\n",
        "    #'isn',\n",
        "    'it',\n",
        "    'its',\n",
        "    'itself',\n",
        "    #'just',\n",
        "    #'keep',\n",
        "    #'last',\n",
        "    'latter',\n",
        "    'latterly',\n",
        "    #'least',\n",
        "    #'less',\n",
        "    'll',\n",
        "    'ltd',\n",
        "    'm',\n",
        "    'ma',\n",
        "    #'made',\n",
        "    #'many',\n",
        "    'may',\n",
        "    'me',\n",
        "    'meanwhile',\n",
        "    'might',\n",
        "    'mightn',\n",
        "    'mill',\n",
        "    'mine',\n",
        "    #'more',\n",
        "    #'moreover',\n",
        "    #'most',\n",
        "    #'mostly',\n",
        "    'move',\n",
        "    'much',\n",
        "    'must',\n",
        "    'mustn',\n",
        "    'my',\n",
        "    'myself',\n",
        "    'name',\n",
        "    #'namely',\n",
        "    #'needn',\n",
        "    #'neither',\n",
        "    #'never',\n",
        "    #'nevertheless',\n",
        "    #'next',\n",
        "    'nine',\n",
        "    'no',\n",
        "    #'nobody',\n",
        "    'none',\n",
        "    'noone',\n",
        "    'nor',\n",
        "    'not',\n",
        "    #'nothing',\n",
        "    'now',\n",
        "    'nowhere',\n",
        "    'o',\n",
        "    'of',\n",
        "    'off',\n",
        "    'often',\n",
        "    'on',\n",
        "    'once',\n",
        "    'one',\n",
        "    'only',\n",
        "    'onto',\n",
        "    'or',\n",
        "    'other',\n",
        "    #'others',\n",
        "    #'otherwise',\n",
        "    'our',\n",
        "    'ours',\n",
        "    'ourselves',\n",
        "    'out',\n",
        "    'over',\n",
        "    'own',\n",
        "    'part',\n",
        "    'per',\n",
        "    'perhaps',\n",
        "    'please',\n",
        "    'put',\n",
        "    #'rather',\n",
        "    're',\n",
        "    's',\n",
        "    'same',\n",
        "    #'see',\n",
        "    #'seemed',\n",
        "    #'seeming',\n",
        "    #'seems',\n",
        "    #'serious',\n",
        "    #'several',\n",
        "    'shan',\n",
        "    'she',\n",
        "    'should',\n",
        "    'shouldn',\n",
        "    'show',\n",
        "    'side',\n",
        "    'since',\n",
        "    'sincere',\n",
        "    'six',\n",
        "    'sixty',\n",
        "    'so',\n",
        "    'some',\n",
        "    'somehow',\n",
        "    'someone',\n",
        "    'something',\n",
        "    'sometime',\n",
        "    'sometimes',\n",
        "    'somewhere',\n",
        "    'still',\n",
        "    'such',\n",
        "    'system',\n",
        "    't',\n",
        "    'take',\n",
        "    'ten',\n",
        "    'than',\n",
        "    'that',\n",
        "    'the',\n",
        "    'their',\n",
        "    'theirs',\n",
        "    'them',\n",
        "    'themselves',\n",
        "    'then',\n",
        "    'thence',\n",
        "    'there',\n",
        "    'thereafter',\n",
        "    'thereby',\n",
        "    'therefore',\n",
        "    'therein',\n",
        "    'thereupon',\n",
        "    'these',\n",
        "    'they',\n",
        "    'thick',\n",
        "    'thin',\n",
        "    'third',\n",
        "    'this',\n",
        "    'those',\n",
        "    'though',\n",
        "    'three',\n",
        "    'through',\n",
        "    'throughout',\n",
        "    'thru',\n",
        "    'thus',\n",
        "    'to',\n",
        "    'together',\n",
        "    'too',\n",
        "    #'top',\n",
        "    'toward',\n",
        "    'towards',\n",
        "    'twelve',\n",
        "    'twenty',\n",
        "    'two',\n",
        "    'un',\n",
        "    'under',\n",
        "    'until',\n",
        "    'up',\n",
        "    'u',  #added\n",
        "    'wa', #added\n",
        "    'upon',\n",
        "    'us',\n",
        "    've',\n",
        "    #'very',\n",
        "    'via',\n",
        "    'was',\n",
        "    'wasn',\n",
        "    'we',\n",
        "    'well',\n",
        "    'were',\n",
        "    'weren',\n",
        "    'what',\n",
        "    'whatever',\n",
        "    'when',\n",
        "    'whence',\n",
        "    'whenever',\n",
        "    'where',\n",
        "    'whereafter',\n",
        "    'whereas',\n",
        "    'whereby',\n",
        "    'wherein',\n",
        "    'whereupon',\n",
        "    'wherever',\n",
        "    'whether',\n",
        "    'which',\n",
        "    'while',\n",
        "    'whither',\n",
        "    'who',\n",
        "    'whoever',\n",
        "    'whole',\n",
        "    'whom',\n",
        "    'whose',\n",
        "    'why',\n",
        "    'will',\n",
        "    'with',\n",
        "    'within',\n",
        "    'without',\n",
        "    'won',\n",
        "    #'would',\n",
        "    #'wouldn',\n",
        "    'y',\n",
        "    'yet',\n",
        "    'you',\n",
        "    'your',\n",
        "    'yours',\n",
        "    'yourself',\n",
        "    'yourselves'\n",
        "]\n",
        "\n",
        "# Note; Need to write a helper function to get the manual features included into the sklearn vectorizer class. Will have to do it in a way\n",
        "# that allows those features to avoid the cutoffs due to min/max document frequency//top N features. If we really need to,\n",
        "# we can do a very roundabout way by making another vectorizer that is run only on text with these features in it, then applying\n",
        "# that vectorizer via the .transform() method to our text data, and then concatenating the datasets.\n",
        "MANUAL_FEATURES = [\n",
        "    \"cannot\", \"unconvinced\", \"needs\", \"unclear\", \"already\", \"how will\", \"how do\", \"unlikely\", \"why don't\", \"is not novel work\", \"isn't novel\", \"is novel work\", \"novel work\", \"is novel\", \"an accept\", \"a reject\", \"convincing\", \"interesting\"] # TODO RETURN TO if time\n",
        "class LemmaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return ([self.wnl.lemmatize(t) for t in word_tokenize(doc)])\n",
        "RARE_DOCUMENT_FREQUENCY_THRESHOLD = 1  # For now, will implement pipeline to do hyperparameter optimization on these features\n",
        "MAX_FEATURES = 2000;  #  For now, will implement pipeline to do hyperparameter optimization on these features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys6exYQ_zWaI"
      },
      "outputs": [],
      "source": [
        "def read_data(path):\n",
        "  df = pd.read_csv(path)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjFjC2P7CtRg"
      },
      "outputs": [],
      "source": [
        "def preprocess(df, column):\n",
        "  df = df.dropna(subset=[column]).reset_index(drop=True)\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  cleaned = []\n",
        "  for review in df[column]:\n",
        "    temp = review.lower()\n",
        "    temp = temp.replace('\\n', ' ')\n",
        "    temp = temp.replace(',', ' ')\n",
        "    temp = temp.replace('.', ' ')\n",
        "    temp = temp.replace('!', ' ')\n",
        "    temp = temp.replace('?', ' ')\n",
        "    temp = temp.replace(':', ' ')\n",
        "    temp = temp.replace(';', ' ')\n",
        "    temp = [lemmatizer.lemmatize(word) for word in temp.split() if word not in stop_words]\n",
        "    temp = ' '.join(temp)\n",
        "    cleaned.append(temp)\n",
        "  df[column] = cleaned\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8qtU86-mJC1"
      },
      "outputs": [],
      "source": [
        "data_as_given = read_data('data_reviews_filtered.csv')\n",
        "data_as_given = preprocess(data_as_given, 'comments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh60iNHQ-ppo"
      },
      "outputs": [],
      "source": [
        "data_as_given['accepted'] = data_as_given['accepted'].astype(str).str.strip().str.capitalize()\n",
        "data_as_given['accepted'] = data_as_given['accepted'].map({'True': 1, 'False': 0})\n",
        "data_as_given.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D0N_1WrX9r9"
      },
      "outputs": [],
      "source": [
        "data_as_given[\"accepted\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxJR0yszmYv_"
      },
      "outputs": [],
      "source": [
        "x_data = data_as_given['comments']\n",
        "y_data = data_as_given['accepted']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "ct_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=ENGLISH_STOP_WORDS_MODIFIED, min_df=RARE_DOCUMENT_FREQUENCY_THRESHOLD, max_features=MAX_FEATURES, ngram_range=(1,3))\n",
        "tf_vect = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words = ENGLISH_STOP_WORDS_MODIFIED, min_df=RARE_DOCUMENT_FREQUENCY_THRESHOLD, max_features=MAX_FEATURES, ngram_range=(1,3), use_idf=False)\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words = ENGLISH_STOP_WORDS_MODIFIED, min_df=RARE_DOCUMENT_FREQUENCY_THRESHOLD, max_features=MAX_FEATURES, ngram_range=(1,3), use_idf=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRyU_wmwy8LR"
      },
      "outputs": [],
      "source": [
        "def svm_train(df, vectorizer, x_train, x_test, y_train, y_test, C, coef0, degree, gamma, kernel):\n",
        "  x_train = vectorizer.fit_transform(x_train)\n",
        "  x_test = vectorizer.transform(x_test)\n",
        "\n",
        "  svm = SVC(C = C, coef0 = coef0, degree = degree, gamma = gamma, kernel=kernel)\n",
        "  svm.fit(x_train, y_train)\n",
        "  y_pred = svm.predict(x_test)\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "  cv_results_f1 = cross_val_score(svm, x_train, y_train, cv=5, scoring='f1')\n",
        "  print(\"CV F1 Score:\", cv_results_f1.mean())\n",
        "  cv_results_acc = cross_val_score(svm, x_train, y_train, cv=5, scoring='accuracy')\n",
        "  print(\"CV Accuracy Score:\", cv_results_acc.mean())\n",
        "\n",
        "  return svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxLDLaDGugZ3"
      },
      "outputs": [],
      "source": [
        "def rf_train(df, vectorizer, x_train, x_test, y_train, y_test, n_estimators, criterion, max_features, max_depth, bootstrap):\n",
        "  x_train = vectorizer.fit_transform(x_train)\n",
        "  x_test = vectorizer.transform(x_test)\n",
        "\n",
        "  rf = RandomForestClassifier(class_weight='balanced', n_estimators= n_estimators, criterion= criterion, max_features= max_features, max_depth= max_depth, bootstrap= bootstrap)\n",
        "  rf.fit(x_train, y_train)\n",
        "  y_pred = rf.predict(x_test)\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "  cv_results_f1 = cross_val_score(rf, x_train, y_train, cv=5, scoring='f1')\n",
        "  print(\"CV F1 Score:\", cv_results_f1.mean())\n",
        "  cv_results_acc = cross_val_score(rf, x_train, y_train, cv=5, scoring='accuracy')\n",
        "  print(\"CV Accuracy Score:\", cv_results_acc.mean())\n",
        "\n",
        "  return rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz11fm3VukW2"
      },
      "outputs": [],
      "source": [
        "def lr_train(df, vectorizer, x_train, x_test, y_train, y_test, penalty, C, max_iter, solver):\n",
        "  x_train = vectorizer.fit_transform(x_train)\n",
        "  x_test = vectorizer.transform(x_test)\n",
        "\n",
        "  lr = LogisticRegression(penalty = penalty, C = C, max_iter = max_iter, solver = solver)\n",
        "  lr.fit(x_train, y_train)\n",
        "  y_pred = lr.predict(x_test)\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  cv_results_f1 = cross_val_score(lr, x_train, y_train, cv=5, scoring='f1')\n",
        "  print(\"CV F1 Score:\", cv_results_f1.mean())\n",
        "  cv_results_acc = cross_val_score(lr, x_train, y_train, cv=5, scoring='accuracy')\n",
        "  print(\"CV Accuracy Score:\", cv_results_acc.mean())\n",
        "\n",
        "  return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d12hfig03kP"
      },
      "outputs": [],
      "source": [
        "def grid_search(svm, x_train, y_train):\n",
        "  param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], 'gamma': ['scale', 'auto'], 'degree': [2, 3, 4, 5], 'coef0': [0.0, 0.1, 0.5, 1.0]}\n",
        "  grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "\n",
        "  grid_search.fit(x_train, y_train)\n",
        "  print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pha5v5krHcXf"
      },
      "outputs": [],
      "source": [
        "def grid_search_rf(rf, x_train, y_train):\n",
        "  param_grid = {\n",
        "    'n_estimators': [25, 40, 50],\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_features': ['log2', 'sqrt'],\n",
        "    'max_depth': [2,4],\n",
        "    'bootstrap': [True, False]\n",
        "    }\n",
        "  grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
        "\n",
        "  grid_search.fit(x_train, y_train)\n",
        "  print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qskmQgVALREO"
      },
      "outputs": [],
      "source": [
        "def grid_search_lr(rf, x_train, y_train):\n",
        "  param_grid = {'penalty':['l2'],'C':[1, 10, 100], 'max_iter': [100, 200, 500, 1000], 'solver': ['lbfgs', 'saga', 'liblinear']}\n",
        "  grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
        "\n",
        "  grid_search.fit(x_train, y_train)\n",
        "  print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbQi4HXMsA0U"
      },
      "outputs": [],
      "source": [
        "svm_ct = svm_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 10, 1.0, 2, 'auto', 'poly')\n",
        "svm_tf = svm_train(data_as_given, tf_vect, x_train, x_test, y_train, y_test, 10, 1.0, 2, 'auto', 'poly')\n",
        "svm_tfidf = svm_train(data_as_given, tfidf_vect, x_train, x_test, y_train, y_test, 10, 1.0, 2, 'auto', 'poly')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQHIs0RXs5C9"
      },
      "outputs": [],
      "source": [
        "x_train_ct = ct_vectorizer.fit_transform(x_train)\n",
        "x_train_tf = tf_vect.fit_transform(x_train)\n",
        "x_train_idf = tfidf_vect.fit_transform(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCSrtlHyYi0p"
      },
      "outputs": [],
      "source": [
        "grid_search(svm_ct, x_train_ct, y_train)\n",
        "grid_search(svm_tf, x_train_tf, y_train)\n",
        "grid_search(svm_tfidf, x_train_idf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqvx2Cc6FX7I"
      },
      "outputs": [],
      "source": [
        "print(\"ct\")\n",
        "svm_ct_tuned = svm_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 10, 1.0, 2, 'auto', 'poly')\n",
        "print(\"tf\")\n",
        "svm_tf_tuned = svm_train(data_as_given, tf_vect, x_train, x_test, y_train, y_test, 1, 1.0, 3, 'scale', 'poly')\n",
        "print(\"tfidf\")\n",
        "svm_tfidf_tuned = svm_train(data_as_given, tfidf_vect, x_train, x_test, y_train, y_test, 10, 0.0, 2, 'scale', 'poly')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkxJxITC_lOM"
      },
      "outputs": [],
      "source": [
        "rf_ct = rf_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 25, 'gini', 'sqrt', 2, False)\n",
        "rf_tf = rf_train(data_as_given, tf_vect, x_train, x_test, y_train, y_test, 25, 'gini', 'sqrt', 2, False)\n",
        "rf_tfidf = rf_train(data_as_given, tfidf_vect, x_train, x_test, y_train, y_test, 25, 'gini', 'sqrt', 2, False)\n",
        "\n",
        "grid_search_rf(rf_ct, x_train_ct, y_train)\n",
        "grid_search_rf(rf_tf, x_train_tf, y_train)\n",
        "grid_search_rf(rf_tfidf, x_train_idf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWtAnEYfsdVQ"
      },
      "outputs": [],
      "source": [
        "print(\"ct\")\n",
        "rf_ct = rf_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 40, 'gini', 'log2', 4, False)\n",
        "print(\"tf\")\n",
        "rf_tf = rf_train(data_as_given, tf_vect, x_train, x_test, y_train, y_test, 40, 'gini', 'sqrt', 2, True)\n",
        "print(\"tfidf\")\n",
        "rf_tfidf = rf_train(data_as_given, tfidf_vect, x_train, x_test, y_train, y_test, 50, 'log_loss', 'sqrt', 4, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqLXAJRZvCS4"
      },
      "outputs": [],
      "source": [
        "lr_ct = lr_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, None, 10, 100, 'saga')\n",
        "lr_tf = lr_train(data_as_given, tf_vect, x_train, x_test, y_train, y_test, None, 10, 100, 'saga')\n",
        "lr_tfidf = lr_train(data_as_given, tfidf_vect, x_train, x_test, y_train, y_test, None, 10, 100, 'saga')\n",
        "\n",
        "grid_search_lr(lr_ct, x_train_ct, y_train)\n",
        "grid_search_lr(lr_tf, x_train_tf, y_train)\n",
        "grid_search_lr(lr_tfidf, x_train_idf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgb-kXRJs60k"
      },
      "outputs": [],
      "source": [
        "print(\"ct\")\n",
        "lr_ct = lr_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 'l2', 1, 1000, 'saga')\n",
        "print(\"tf\")\n",
        "lr_tf = lr_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 'l2', 10, 200, 'saga')\n",
        "print(\"tfidf\")\n",
        "lr_tfidf = lr_train(data_as_given, ct_vectorizer, x_train, x_test, y_train, y_test, 'l2', 10, 100, 'saga')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoGqf1OdbqeM"
      },
      "outputs": [],
      "source": [
        "coefs=lr_ct.coef_[0]\n",
        "top_three_ct = np.argpartition(coefs, -3)[-3:]\n",
        "print(data_as_given.feature_names[top_three])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

# Predicting_Paper_Acceptance_Status_from_Peer_Reviews
Group project with [Pleasant Ballenger](https://www.linkedin.com/in/pleasant-ballenger/), [Anna Emsbach](https://www.linkedin.com/in/anna-emsbach-949464298/?originalSubdomain=de) and [Manne Wennberg](https://www.linkedin.com/in/manne-wennberg?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAADiyVaMBwB36fAFDZlejT8lKj2g1vyzbpCk&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_people%3BlmIagg3nT6C8u2fD6aJN9A%3D%3D) for our COMP 562 Machine learning course. In the vein of [Kang et al. 2018](https://github.com/allenai/PeerRead), we used the PeerRead dataset from the conference papers submitted to ICLR 2017 to build a few models to predict whether a paper would be accepted or rejected solely from 'reading' the peer reviews. We focused on the text of the reviews to provide an application that would automize the role that an editor plays in synthesizing peer reviews. For that reason, we aimed to exclude title, abstract, body text, editor comments, and author replies to reviews from our training data. Resulting model performance (SVM, logisitic regression, random forests) performed better than benchmark, once you correct for data errors that were not cleaned in the PeerRead group's paper (see [PeerAssist](https://link.springer.com/chapter/10.1007/978-3-030-91669-5_33) to explain)
